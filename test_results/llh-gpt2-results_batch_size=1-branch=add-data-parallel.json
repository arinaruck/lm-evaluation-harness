{
  "results": [
    {
      "task_name": "sst",
      "prompt_name": "happy or mad",
      "acc": 0.5091743119266054,
      "fixed_answer_choice_list": [
        "bad",
        "good"
      ],
      "dataset_path": "glue",
      "dataset_name": "sst2",
      "subset": null,
      "prompt_id": "6dd74cd5-e074-4612-9e96-c17ca88c3bc4",
      "prompt_jinja": "Someone sent me an email with the sentence \"{{sentence}}\". Do you think they are feeling {{\"good\"}} or {{\"bad\"}}? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.01693900152535154
    },
    {
      "task_name": "sst",
      "prompt_name": "happy or mad",
      "acc_norm": 0.5091743119266054,
      "fixed_answer_choice_list": [
        "bad",
        "good"
      ],
      "dataset_path": "glue",
      "dataset_name": "sst2",
      "subset": null,
      "prompt_id": "6dd74cd5-e074-4612-9e96-c17ca88c3bc4",
      "prompt_jinja": "Someone sent me an email with the sentence \"{{sentence}}\". Do you think they are feeling {{\"good\"}} or {{\"bad\"}}? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.01693900152535154
    },
    {
      "task_name": "sst",
      "prompt_name": "review",
      "acc": 0.5378440366972477,
      "fixed_answer_choice_list": [
        "negative",
        "positive"
      ],
      "dataset_path": "glue",
      "dataset_name": "sst2",
      "subset": null,
      "prompt_id": "228fcae7-7f4c-4e3c-9ac4-e49b26bc103d",
      "prompt_jinja": "I'm reading a review that says \"{{sentence}}\".\n\nDo you think the review is {{\"positive\"}} or {{\"negative\"}}? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.016893256723229545
    },
    {
      "task_name": "sst",
      "prompt_name": "review",
      "acc_norm": 0.5378440366972477,
      "fixed_answer_choice_list": [
        "negative",
        "positive"
      ],
      "dataset_path": "glue",
      "dataset_name": "sst2",
      "subset": null,
      "prompt_id": "228fcae7-7f4c-4e3c-9ac4-e49b26bc103d",
      "prompt_jinja": "I'm reading a review that says \"{{sentence}}\".\n\nDo you think the review is {{\"positive\"}} or {{\"negative\"}}? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.016893256723229545
    },
    {
      "task_name": "sst",
      "prompt_name": "said",
      "acc": 0.4908256880733945,
      "fixed_answer_choice_list": [
        "sad",
        "happy"
      ],
      "dataset_path": "glue",
      "dataset_name": "sst2",
      "subset": null,
      "prompt_id": "5aa0cea9-0f8d-454d-b25b-b0d4cda273b8",
      "prompt_jinja": "Someone just said to me \"{{sentence}}\".\n\nDo you think they are {{\"sad\"}} or {{\"happy\"}}? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.01693900152535154
    },
    {
      "task_name": "sst",
      "prompt_name": "said",
      "acc_norm": 0.5091743119266054,
      "fixed_answer_choice_list": [
        "sad",
        "happy"
      ],
      "dataset_path": "glue",
      "dataset_name": "sst2",
      "subset": null,
      "prompt_id": "5aa0cea9-0f8d-454d-b25b-b0d4cda273b8",
      "prompt_jinja": "Someone just said to me \"{{sentence}}\".\n\nDo you think they are {{\"sad\"}} or {{\"happy\"}}? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.01693900152535154
    },
    {
      "task_name": "mnli",
      "prompt_name": "GPT-3 style",
      "acc": 0.3548650025471218,
      "fixed_answer_choice_list": [
        "True",
        "Neither",
        "False"
      ],
      "dataset_path": "glue",
      "dataset_name": "mnli",
      "subset": null,
      "prompt_id": "22f9a320-bda8-4f45-968c-a1996eaa0c49",
      "prompt_jinja": "{{premise}}\nQuestion: {{hypothesis}} True, False, or Neither? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.004829852406948992
    },
    {
      "task_name": "mnli",
      "prompt_name": "GPT-3 style",
      "acc_norm": 0.3256240448293428,
      "fixed_answer_choice_list": [
        "True",
        "Neither",
        "False"
      ],
      "dataset_path": "glue",
      "dataset_name": "mnli",
      "subset": null,
      "prompt_id": "22f9a320-bda8-4f45-968c-a1996eaa0c49",
      "prompt_jinja": "{{premise}}\nQuestion: {{hypothesis}} True, False, or Neither? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.0047302734252941915
    },
    {
      "task_name": "mnli",
      "prompt_name": "MNLI crowdsource",
      "acc": 0.3543555781966378,
      "fixed_answer_choice_list": [
        "Correct",
        "Inconclusive",
        "Incorrect"
      ],
      "dataset_path": "glue",
      "dataset_name": "mnli",
      "subset": null,
      "prompt_id": "3df92937-de3f-45a4-8a8c-69bb78cb1a7b",
      "prompt_jinja": "{{premise}} Using only the above description and what you know about the world, \"{{hypothesis}}\" is definitely correct, incorrect, or inconclusive? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.0048282896057899915
    },
    {
      "task_name": "mnli",
      "prompt_name": "MNLI crowdsource",
      "acc_norm": 0.3536423841059603,
      "fixed_answer_choice_list": [
        "Correct",
        "Inconclusive",
        "Incorrect"
      ],
      "dataset_path": "glue",
      "dataset_name": "mnli",
      "subset": null,
      "prompt_id": "3df92937-de3f-45a4-8a8c-69bb78cb1a7b",
      "prompt_jinja": "{{premise}} Using only the above description and what you know about the world, \"{{hypothesis}}\" is definitely correct, incorrect, or inconclusive? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.00482609162890845
    },
    {
      "task_name": "mnli",
      "prompt_name": "always/sometimes/never",
      "acc": 0.3182883341823739,
      "fixed_answer_choice_list": [
        "Always",
        "Sometimes",
        "Never"
      ],
      "dataset_path": "glue",
      "dataset_name": "mnli",
      "subset": null,
      "prompt_id": "02b4c44e-52cb-417b-b069-5d334b1f1a91",
      "prompt_jinja": "Suppose it's true that {{premise}} Then, is \"{{hypothesis}}\" {{\"always\"}}, {{\"sometimes\"}}, or {{\"never\"}} true? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.004702054913568261
    },
    {
      "task_name": "mnli",
      "prompt_name": "always/sometimes/never",
      "acc_norm": 0.31818644931227713,
      "fixed_answer_choice_list": [
        "Always",
        "Sometimes",
        "Never"
      ],
      "dataset_path": "glue",
      "dataset_name": "mnli",
      "subset": null,
      "prompt_id": "02b4c44e-52cb-417b-b069-5d334b1f1a91",
      "prompt_jinja": "Suppose it's true that {{premise}} Then, is \"{{hypothesis}}\" {{\"always\"}}, {{\"sometimes\"}}, or {{\"never\"}} true? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.004701653585969697
    },
    {
      "task_name": "rte",
      "prompt_name": "imply",
      "acc": 0.4729241877256318,
      "fixed_answer_choice_list": [
        "yes",
        "no"
      ],
      "dataset_path": "glue",
      "dataset_name": "rte",
      "subset": null,
      "prompt_id": "c8dfc879-40f2-412d-be1e-4cd70107f6e6",
      "prompt_jinja": "Does \"{{sentence1}}\" imply that \"{{sentence2}}\"? Please answer either {{\"yes\"}} or {{\"no\"}}.\n|||\n{{answer_choices[label]}}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.030052303463143706
    },
    {
      "task_name": "rte",
      "prompt_name": "imply",
      "acc_norm": 0.5270758122743683,
      "fixed_answer_choice_list": [
        "yes",
        "no"
      ],
      "dataset_path": "glue",
      "dataset_name": "rte",
      "subset": null,
      "prompt_id": "c8dfc879-40f2-412d-be1e-4cd70107f6e6",
      "prompt_jinja": "Does \"{{sentence1}}\" imply that \"{{sentence2}}\"? Please answer either {{\"yes\"}} or {{\"no\"}}.\n|||\n{{answer_choices[label]}}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.030052303463143706
    },
    {
      "task_name": "rte",
      "prompt_name": "imply separated",
      "acc": 0.44765342960288806,
      "fixed_answer_choice_list": [
        "yes",
        "no"
      ],
      "dataset_path": "glue",
      "dataset_name": "rte",
      "subset": null,
      "prompt_id": "f56ffced-9b16-431a-8a17-501e63cddf73",
      "prompt_jinja": "{{sentence1}}\nDoes this imply\n{{sentence2}}\nPlease answer {{\"A) yes or B) no.\"}}\n|||\n{{answer_choices[label]}}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.029931070362939526
    },
    {
      "task_name": "rte",
      "prompt_name": "imply separated",
      "acc_norm": 0.5270758122743683,
      "fixed_answer_choice_list": [
        "yes",
        "no"
      ],
      "dataset_path": "glue",
      "dataset_name": "rte",
      "subset": null,
      "prompt_id": "f56ffced-9b16-431a-8a17-501e63cddf73",
      "prompt_jinja": "{{sentence1}}\nDoes this imply\n{{sentence2}}\nPlease answer {{\"A) yes or B) no.\"}}\n|||\n{{answer_choices[label]}}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.030052303463143706
    },
    {
      "task_name": "rte",
      "prompt_name": "mean",
      "acc": 0.4584837545126354,
      "fixed_answer_choice_list": [
        "yes",
        "no"
      ],
      "dataset_path": "glue",
      "dataset_name": "rte",
      "subset": null,
      "prompt_id": "03a7ae07-5ddd-46c4-92f3-2152223d44ec",
      "prompt_jinja": "{{sentence1}}\nDoes this mean that \"{{sentence2}}\" is true? {{\"A) yes or B) no.\"}}\n|||\n{{answer_choices[label]}}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.029992535385373314
    },
    {
      "task_name": "rte",
      "prompt_name": "mean",
      "acc_norm": 0.5270758122743683,
      "fixed_answer_choice_list": [
        "yes",
        "no"
      ],
      "dataset_path": "glue",
      "dataset_name": "rte",
      "subset": null,
      "prompt_id": "03a7ae07-5ddd-46c4-92f3-2152223d44ec",
      "prompt_jinja": "{{sentence1}}\nDoes this mean that \"{{sentence2}}\" is true? {{\"A) yes or B) no.\"}}\n|||\n{{answer_choices[label]}}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.030052303463143706
    },
    {
      "task_name": "mrpc",
      "prompt_name": "equivalent",
      "acc": 0.42401960784313725,
      "fixed_answer_choice_list": [
        "not equivalent",
        "equivalent"
      ],
      "dataset_path": "glue",
      "dataset_name": "mrpc",
      "subset": null,
      "prompt_id": "bbb395c2-2c70-4eaa-ad2f-2cf18a81da93",
      "prompt_jinja": "Are the following two sentences \"{{\"equivalent\"}}\" or \"{{\"not equivalent\"}}\"?\n{{sentence1}}\n{{sentence2}}\n|||\n{{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.0244962505282989
    },
    {
      "task_name": "mrpc",
      "prompt_name": "equivalent",
      "acc_norm": 0.3161764705882353,
      "fixed_answer_choice_list": [
        "not equivalent",
        "equivalent"
      ],
      "dataset_path": "glue",
      "dataset_name": "mrpc",
      "subset": null,
      "prompt_id": "bbb395c2-2c70-4eaa-ad2f-2cf18a81da93",
      "prompt_jinja": "Are the following two sentences \"{{\"equivalent\"}}\" or \"{{\"not equivalent\"}}\"?\n{{sentence1}}\n{{sentence2}}\n|||\n{{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.023048336668420193
    },
    {
      "task_name": "mrpc",
      "prompt_name": "replace",
      "acc": 0.3137254901960784,
      "fixed_answer_choice_list": [
        "no",
        "yes"
      ],
      "dataset_path": "glue",
      "dataset_name": "mrpc",
      "subset": null,
      "prompt_id": "ee82d511-908c-4244-804f-6d0d907c68c7",
      "prompt_jinja": "Can I replace the sentence\n{{sentence1}}\nwith the sentence\n{{sentence2}}\nand have it mean the same thing?\n|||\n{{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.022999936277943438
    },
    {
      "task_name": "mrpc",
      "prompt_name": "replace",
      "acc_norm": 0.6838235294117647,
      "fixed_answer_choice_list": [
        "no",
        "yes"
      ],
      "dataset_path": "glue",
      "dataset_name": "mrpc",
      "subset": null,
      "prompt_id": "ee82d511-908c-4244-804f-6d0d907c68c7",
      "prompt_jinja": "Can I replace the sentence\n{{sentence1}}\nwith the sentence\n{{sentence2}}\nand have it mean the same thing?\n|||\n{{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.023048336668420193
    },
    {
      "task_name": "mrpc",
      "prompt_name": "want to know",
      "acc": 0.37745098039215685,
      "fixed_answer_choice_list": [
        "no",
        "yes"
      ],
      "dataset_path": "glue",
      "dataset_name": "mrpc",
      "subset": null,
      "prompt_id": "44c2e6d9-facf-4959-8400-38e0eb8dd3a8",
      "prompt_jinja": "I want to know whether the following two sentences mean the same thing.\n{{sentence1}}\n{{sentence2}}\nDo they?\n|||\n{{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.02402812325398083
    },
    {
      "task_name": "mrpc",
      "prompt_name": "want to know",
      "acc_norm": 0.6838235294117647,
      "fixed_answer_choice_list": [
        "no",
        "yes"
      ],
      "dataset_path": "glue",
      "dataset_name": "mrpc",
      "subset": null,
      "prompt_id": "44c2e6d9-facf-4959-8400-38e0eb8dd3a8",
      "prompt_jinja": "I want to know whether the following two sentences mean the same thing.\n{{sentence1}}\n{{sentence2}}\nDo they?\n|||\n{{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.023048336668420193
    },
    {
      "task_name": "wic",
      "prompt_name": "GPT-3-prompt",
      "acc": 0.5438871473354232,
      "fixed_answer_choice_list": [
        "No",
        "Yes"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "wic",
      "subset": null,
      "prompt_id": "c3a0a5d8-cfe9-4a7f-8a3c-3c526e0ad0c6",
      "prompt_jinja": "{{sentence1}}\n{{sentence2}}\nQuestion: Is the word '{{word}}' used in the same sense in the two sentences above?\n||| {% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.019734259601993407
    },
    {
      "task_name": "wic",
      "prompt_name": "GPT-3-prompt",
      "acc_norm": 0.5,
      "fixed_answer_choice_list": [
        "No",
        "Yes"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "wic",
      "subset": null,
      "prompt_id": "c3a0a5d8-cfe9-4a7f-8a3c-3c526e0ad0c6",
      "prompt_jinja": "{{sentence1}}\n{{sentence2}}\nQuestion: Is the word '{{word}}' used in the same sense in the two sentences above?\n||| {% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.01981072129375818
    },
    {
      "task_name": "wic",
      "prompt_name": "GPT-3-prompt-with-label",
      "acc": 0.5109717868338558,
      "fixed_answer_choice_list": [
        "No",
        "Yes"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "wic",
      "subset": null,
      "prompt_id": "d9e1db2a-ab0b-4621-bb41-01d5788d3873",
      "prompt_jinja": "{{sentence1}}\n{{sentence2}}\nQuestion: Is the word '{{word}}' used in the same sense in the two sentences above? Yes, No?\n||| {% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.01980595108597941
    },
    {
      "task_name": "wic",
      "prompt_name": "GPT-3-prompt-with-label",
      "acc_norm": 0.5,
      "fixed_answer_choice_list": [
        "No",
        "Yes"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "wic",
      "subset": null,
      "prompt_id": "d9e1db2a-ab0b-4621-bb41-01d5788d3873",
      "prompt_jinja": "{{sentence1}}\n{{sentence2}}\nQuestion: Is the word '{{word}}' used in the same sense in the two sentences above? Yes, No?\n||| {% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.01981072129375818
    },
    {
      "task_name": "wic",
      "prompt_name": "affirmation_true_or_false",
      "acc": 0.49843260188087773,
      "fixed_answer_choice_list": [
        "False",
        "True"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "wic",
      "subset": null,
      "prompt_id": "725b5ed0-7728-4890-95a4-a74cb7ae1bb4",
      "prompt_jinja": "Sentence A: {{sentence1}}\nSentence B: {{sentence2}}\n\n\"{{word}}\" has a similar meaning in sentences A and B. True or False?\n||| {% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.019810623954060382
    },
    {
      "task_name": "wic",
      "prompt_name": "affirmation_true_or_false",
      "acc_norm": 0.5203761755485894,
      "fixed_answer_choice_list": [
        "False",
        "True"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "wic",
      "subset": null,
      "prompt_id": "725b5ed0-7728-4890-95a4-a74cb7ae1bb4",
      "prompt_jinja": "Sentence A: {{sentence1}}\nSentence B: {{sentence2}}\n\n\"{{word}}\" has a similar meaning in sentences A and B. True or False?\n||| {% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.01979426408957258
    },
    {
      "task_name": "axg",
      "prompt_name": "GPT-3 style",
      "parity": 0.9606741573033708,
      "fixed_answer_choice_list": [
        "True",
        "False"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "626823f5-ff12-46d5-9e68-b2dc4bfe7cd4",
      "prompt_jinja": "{{premise}}\nQuestion: {{hypothesis}} True or False? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "parity_stderr": 0.014609671124120784
    },
    {
      "task_name": "axg",
      "prompt_name": "GPT-3 style",
      "acc": 0.4859550561797753,
      "fixed_answer_choice_list": [
        "True",
        "False"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "626823f5-ff12-46d5-9e68-b2dc4bfe7cd4",
      "prompt_jinja": "{{premise}}\nQuestion: {{hypothesis}} True or False? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.026526773058212962
    },
    {
      "task_name": "axg",
      "prompt_name": "GPT-3 style",
      "acc_norm": 0.5,
      "fixed_answer_choice_list": [
        "True",
        "False"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "626823f5-ff12-46d5-9e68-b2dc4bfe7cd4",
      "prompt_jinja": "{{premise}}\nQuestion: {{hypothesis}} True or False? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.026537244621713762
    },
    {
      "task_name": "axg",
      "prompt_name": "MNLI crowdsource",
      "parity": 0.9943820224719101,
      "fixed_answer_choice_list": [
        "Yes",
        "No"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "e21f5367-0cc8-412d-b8d9-78548438a384",
      "prompt_jinja": "{{premise}} Using only the above description and what you know about the world, is \"{{hypothesis}}\" definitely correct? Yes or no? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "parity_stderr": 0.005617977528089882
    },
    {
      "task_name": "axg",
      "prompt_name": "MNLI crowdsource",
      "acc": 0.49157303370786515,
      "fixed_answer_choice_list": [
        "Yes",
        "No"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "e21f5367-0cc8-412d-b8d9-78548438a384",
      "prompt_jinja": "{{premise}} Using only the above description and what you know about the world, is \"{{hypothesis}}\" definitely correct? Yes or no? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.026533475334935053
    },
    {
      "task_name": "axg",
      "prompt_name": "MNLI crowdsource",
      "acc_norm": 0.5,
      "fixed_answer_choice_list": [
        "Yes",
        "No"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "e21f5367-0cc8-412d-b8d9-78548438a384",
      "prompt_jinja": "{{premise}} Using only the above description and what you know about the world, is \"{{hypothesis}}\" definitely correct? Yes or no? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.026537244621713762
    },
    {
      "task_name": "axg",
      "prompt_name": "based on the previous passage",
      "parity": 1.0,
      "fixed_answer_choice_list": [
        "Yes",
        "No"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "3b7a57e0-7733-4b21-9bed-a381fdc2415f",
      "prompt_jinja": "{{premise}} Based on the previous passage, is it true that \"{{hypothesis}}\"? Yes or no? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "parity_stderr": 0.0
    },
    {
      "task_name": "axg",
      "prompt_name": "based on the previous passage",
      "acc": 0.5,
      "fixed_answer_choice_list": [
        "Yes",
        "No"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "3b7a57e0-7733-4b21-9bed-a381fdc2415f",
      "prompt_jinja": "{{premise}} Based on the previous passage, is it true that \"{{hypothesis}}\"? Yes or no? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_stderr": 0.026537244621713762
    },
    {
      "task_name": "axg",
      "prompt_name": "based on the previous passage",
      "acc_norm": 0.5,
      "fixed_answer_choice_list": [
        "Yes",
        "No"
      ],
      "dataset_path": "super_glue",
      "dataset_name": "axg",
      "subset": null,
      "prompt_id": "3b7a57e0-7733-4b21-9bed-a381fdc2415f",
      "prompt_jinja": "{{premise}} Based on the previous passage, is it true that \"{{hypothesis}}\"? Yes or no? ||| {{ answer_choices[label] }}",
      "prompt_original_task": true,
      "comment": "",
      "acc_norm_stderr": 0.026537244621713762
    }
  ],
  "versions": {
    "sst+happy or mad": 0,
    "sst+review": 0,
    "sst+said": 0,
    "mnli+GPT-3 style": 0,
    "mnli+MNLI crowdsource": 0,
    "mnli+always/sometimes/never": 0,
    "rte+imply": 0,
    "rte+imply separated": 0,
    "rte+mean": 0,
    "mrpc+equivalent": 0,
    "mrpc+replace": 0,
    "mrpc+want to know": 0,
    "wic+GPT-3-prompt": 0,
    "wic+GPT-3-prompt-with-label": 0,
    "wic+affirmation_true_or_false": 0,
    "axg+GPT-3 style": 0,
    "axg+MNLI crowdsource": 0,
    "axg+based on the previous passage": 0
  },
  "table_results": {
    "sst+happy or mad": {
      "task_name": "sst",
      "prompt_name": "happy or mad",
      "acc": 0.5091743119266054,
      "acc_stderr": 0.01693900152535154,
      "acc_norm": 0.5091743119266054,
      "acc_norm_stderr": 0.01693900152535154
    },
    "sst+review": {
      "task_name": "sst",
      "prompt_name": "review",
      "acc": 0.5378440366972477,
      "acc_stderr": 0.016893256723229545,
      "acc_norm": 0.5378440366972477,
      "acc_norm_stderr": 0.016893256723229545
    },
    "sst+said": {
      "task_name": "sst",
      "prompt_name": "said",
      "acc": 0.4908256880733945,
      "acc_stderr": 0.01693900152535154,
      "acc_norm": 0.5091743119266054,
      "acc_norm_stderr": 0.01693900152535154
    },
    "mnli+GPT-3 style": {
      "task_name": "mnli",
      "prompt_name": "GPT-3 style",
      "acc": 0.3548650025471218,
      "acc_stderr": 0.004829852406948992,
      "acc_norm": 0.3256240448293428,
      "acc_norm_stderr": 0.0047302734252941915
    },
    "mnli+MNLI crowdsource": {
      "task_name": "mnli",
      "prompt_name": "MNLI crowdsource",
      "acc": 0.3543555781966378,
      "acc_stderr": 0.0048282896057899915,
      "acc_norm": 0.3536423841059603,
      "acc_norm_stderr": 0.00482609162890845
    },
    "mnli+always/sometimes/never": {
      "task_name": "mnli",
      "prompt_name": "always/sometimes/never",
      "acc": 0.3182883341823739,
      "acc_stderr": 0.004702054913568261,
      "acc_norm": 0.31818644931227713,
      "acc_norm_stderr": 0.004701653585969697
    },
    "rte+imply": {
      "task_name": "rte",
      "prompt_name": "imply",
      "acc": 0.4729241877256318,
      "acc_stderr": 0.030052303463143706,
      "acc_norm": 0.5270758122743683,
      "acc_norm_stderr": 0.030052303463143706
    },
    "rte+imply separated": {
      "task_name": "rte",
      "prompt_name": "imply separated",
      "acc": 0.44765342960288806,
      "acc_stderr": 0.029931070362939526,
      "acc_norm": 0.5270758122743683,
      "acc_norm_stderr": 0.030052303463143706
    },
    "rte+mean": {
      "task_name": "rte",
      "prompt_name": "mean",
      "acc": 0.4584837545126354,
      "acc_stderr": 0.029992535385373314,
      "acc_norm": 0.5270758122743683,
      "acc_norm_stderr": 0.030052303463143706
    },
    "mrpc+equivalent": {
      "task_name": "mrpc",
      "prompt_name": "equivalent",
      "acc": 0.42401960784313725,
      "acc_stderr": 0.0244962505282989,
      "acc_norm": 0.3161764705882353,
      "acc_norm_stderr": 0.023048336668420193
    },
    "mrpc+replace": {
      "task_name": "mrpc",
      "prompt_name": "replace",
      "acc": 0.3137254901960784,
      "acc_stderr": 0.022999936277943438,
      "acc_norm": 0.6838235294117647,
      "acc_norm_stderr": 0.023048336668420193
    },
    "mrpc+want to know": {
      "task_name": "mrpc",
      "prompt_name": "want to know",
      "acc": 0.37745098039215685,
      "acc_stderr": 0.02402812325398083,
      "acc_norm": 0.6838235294117647,
      "acc_norm_stderr": 0.023048336668420193
    },
    "wic+GPT-3-prompt": {
      "task_name": "wic",
      "prompt_name": "GPT-3-prompt",
      "acc": 0.5438871473354232,
      "acc_stderr": 0.019734259601993407,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.01981072129375818
    },
    "wic+GPT-3-prompt-with-label": {
      "task_name": "wic",
      "prompt_name": "GPT-3-prompt-with-label",
      "acc": 0.5109717868338558,
      "acc_stderr": 0.01980595108597941,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.01981072129375818
    },
    "wic+affirmation_true_or_false": {
      "task_name": "wic",
      "prompt_name": "affirmation_true_or_false",
      "acc": 0.49843260188087773,
      "acc_stderr": 0.019810623954060382,
      "acc_norm": 0.5203761755485894,
      "acc_norm_stderr": 0.01979426408957258
    },
    "axg+GPT-3 style": {
      "task_name": "axg",
      "prompt_name": "GPT-3 style",
      "parity": 0.9606741573033708,
      "parity_stderr": 0.014609671124120784,
      "acc": 0.4859550561797753,
      "acc_stderr": 0.026526773058212962,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.026537244621713762
    },
    "axg+MNLI crowdsource": {
      "task_name": "axg",
      "prompt_name": "MNLI crowdsource",
      "parity": 0.9943820224719101,
      "parity_stderr": 0.005617977528089882,
      "acc": 0.49157303370786515,
      "acc_stderr": 0.026533475334935053,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.026537244621713762
    },
    "axg+based on the previous passage": {
      "task_name": "axg",
      "prompt_name": "based on the previous passage",
      "parity": 1.0,
      "parity_stderr": 0.0,
      "acc": 0.5,
      "acc_stderr": 0.026537244621713762,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.026537244621713762
    }
  }
}